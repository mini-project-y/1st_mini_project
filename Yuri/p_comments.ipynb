{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DB에 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = pymysql.connect(host=\"localhost\", port=3306, db=\"project\", \n",
    "#                        user=\"humanda\", password=\"humanda\")\n",
    "\n",
    "\n",
    "# sql3 = \"select * from transportation\"\n",
    "\n",
    "# transportation = pd.read_sql_query(sql3, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미세먼지 농도 범위에 따른 데이터프레임 분리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_df = fine_dust[fine_dust['fine_dust(㎍/㎥)'] <= 30]\n",
    "# normal_df = fine_dust[(fine_dust['fine_dust(㎍/㎥)'] > 30) & (fine_dust['fine_dust(㎍/㎥)'] <= 80)]\n",
    "# bad_df = fine_dust[(fine_dust['fine_dust(㎍/㎥)'] > 80) & (fine_dust['fine_dust(㎍/㎥)'] <= 150)]\n",
    "# very_bad_df = fine_dust[fine_dust['fine_dust(㎍/㎥)'] > 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF 조건주기 (np.where or np.select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/\n",
    "\n",
    "# np.where(condition, x, y)는 조건이 충족되면x를, 그렇지 않으면y를 반환\n",
    "# 예제) df[\"Status\"] = np.where(df[\"Salary\"] >= 400, \"Senior\", \"Junior\")\n",
    "# salary가 400 이상이면 status 컬럼에 senior를, 미만이라면 junior를 입력\n",
    "\n",
    "# np.where()는 조건 목록과 선택 목록을 입력으로 받아서 조건에 따라 선택 목록의 요소로 구성된 배열을 반환\n",
    "# 이 방법을 사용하면 두 개 이상의 조건이있을 때 Pandas의 주어진 조건을 기반으로 DataFrame 열을 만듦\n",
    "# 예제\n",
    "# conditionlist = [                                 # 조건들을 담는 변수명 = []\n",
    "#     (df[\"Salary\"] >= 500),                        # 조건1 salary는 500 이상\n",
    "#     (df[\"Salary\"] >= 300) & (df[\"Salary\"] < 300), # 조건2 salary는 300 이상, 300미만?\n",
    "#     (df[\"Salary\"] <= 300),                        # 조건3 salary는 300 이하\n",
    "# ]\n",
    "# choicelist = [\"High\", \"Mid\", \"Low\"]               # 값 이름 변수 = [조건1의 값, 조건2의 값, 조건3의 값]\n",
    "# df[\"Salary_Range\"] = np.select(conditionlist, choicelist, default=\"Not Specified\")\n",
    "# df[\"새로운컬럼이름\"] = np.select(조건변수명, 값이름변수, default=\"조건미충족시 표시할 이름\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미세먼지 등급 매기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 조건 설정\n",
    "# conditions = [\n",
    "#     (fine_dust['fine_dust(㎍/㎥)'] <= 30),\n",
    "#     (fine_dust['fine_dust(㎍/㎥)'] > 30) & (fine_dust['fine_dust(㎍/㎥)'] <= 80),\n",
    "#     (fine_dust['fine_dust(㎍/㎥)'] > 80) & (fine_dust['fine_dust(㎍/㎥)'] <= 150),\n",
    "#     (fine_dust['fine_dust(㎍/㎥)'] > 150)\n",
    "# ]\n",
    "\n",
    "# # 각 조건에 따른 값\n",
    "# values = ['good', 'normal', 'bad', 'very_bad']\n",
    "\n",
    "# # 새로운 'grade' 컬럼 추가 (default 값을 설정해 오류 해결)\n",
    "# fine_dust['grade'] = np.select(conditions, values, default='unknown')\n",
    "\n",
    "# # 결과 확인\n",
    "# fine_dust.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "판다스 datetime 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"신컬럼명\"] = data[\"기존컬럼명\"].dt.date \n",
    "# 기존컬럼에 있는 datetime형식의 날짜를 년월일로 신컬럼에 저장\n",
    "\n",
    "# df[\"신컬럼명\"] = data[\"기존컬럼명\"].dt.year \n",
    "# 기존컬럼에 있는 datetime형식의 날짜를 연(숫자4자리)만 신컬럼에 저장\n",
    "\n",
    "# df[\"신컬럼명\"] = data[\"기존컬럼명\"].dt.month \n",
    "# 기존컬럼에 있는 datetime형식의 날짜를 월(숫자)만 신컬럼에 저장\n",
    "\n",
    "# df[\"신컬럼명\"] = data[\"기존컬럼명\"].dt.month_name \n",
    "# 기존컬럼에 있는 datetime형식의 날짜를 월(문자)만 신컬럼에 저장\n",
    "\n",
    "# df[\"신컬럼명\"] = data[\"기존컬럼명\"].dt.day\n",
    "# 기존컬럼에 있는 datetime형식의 날짜를 일(숫자)만 신컬럼에 저장\n",
    "\n",
    "# 같은 방법으로 dt.hour, dt.minute, dt.second로 시(숫자)/분(숫자)/초(숫자) 나눌 수 있다\n",
    "\n",
    "# dt.weekday 요일(숫자) > 월=0~일6 = dayofweek\n",
    "\n",
    "# dt.quater 분기(숫자), dt.day_name() 요일이름(문자) = day_name(), \n",
    "# dt.weekofyear 연 기준 몇 주째(숫자), dt.dayofyear 연 기준 몇 일째(숫자)\n",
    "# dt.days_in_month 월 일수(숫자) = dayismonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 날짜 컬럼을 datetime 형식으로 변환\n",
    "# fine_dust['date'] = pd.to_datetime(fine_dust['date'])\n",
    "\n",
    "# # 연도, 계절, 평일/주말 정보 추가\n",
    "# fine_dust['year'] = fine_dust['date'].dt.year\n",
    "# fine_dust['weekday'] = np.where(fine_dust['date'].dt.dayofweek < 5, 'weekday', 'weekend')\n",
    "\n",
    "# # 등급과 평일/주말에 따른 데이터 수 세기\n",
    "# count_data = fine_dust.groupby(['grade', 'weekday']).size().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine_dust에 day컬럼을 추가해서 평일/주말(+공휴일)로 값주는 방법\n",
    "- 공휴일 데이터프레임 hd합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1 - 람다해서 일대일매칭해서 맞으면 공휴일및 주말항목으로 넣기\n",
    "# 방법2 - merge or join merge에 how 가 있음, 이거로 inner join\n",
    "# - merge로 컬럼 붙임. 그다음 판단 left outer join 으로 공휴일 넣기 \n",
    "# - 컬럼 추가 + 평일/주말 컬럼도 있음 요 두 컬럼을 조합해서 있으면 휴일 아니면 평일\n",
    "\n",
    "# hd[\"date\"]를 fine_dust[\"hd_date\"] 컬럼으로 추가\n",
    "# 이거 할 때 merge 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hd[\"dateName\"]의 값을 있으면 weekend, 없다면 day컬럼의 값을 따라 쓰고 병합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 방법1 - step by step으로 하기\n",
    "\n",
    "# x1 = merge_date['dateName'].map(lambda v: str(v) != 'nan')\n",
    "# 병합한 데이터 프레임의 dateName의 값을 문자열로 바꾸고, nan이 아닌 것만 x1에 넣기\n",
    "\n",
    "# merge_date['day2'] = merge_date['day']\n",
    "# 병합한 데이터 프레임에 day2컬럼을 만들어서 day의 값을 넣기\n",
    "\n",
    "# merge_date[x1] = 'weekend'\n",
    "# x1시리즈에 true인 값을 weekend로 바꾸기\n",
    "\n",
    "# merge_date[merge_date[\"day2\"] =='weekend']\n",
    "# x1의 weekend값을 day2에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2 - apply활용\n",
    "\n",
    "# merge_date.iloc[:5, :].apply(lambda row : print(row), axis=1)\n",
    "# 병합한 df의 [0~5행, 전체열]에 apply적용해서 출력하기, axis=1이면 행으로 적용한다.\n",
    "        # date              2021-01-01 00:00:00\n",
    "        # fine_dust(㎍/㎥)                   36.0\n",
    "        # grade                          normal\n",
    "        # day                           weekday\n",
    "        # dateName                         1월1일\n",
    "        # Name: 0, dtype: object\n",
    "        # date              2021-01-02 00:00:00\n",
    "        # fine_dust(㎍/㎥)                   43.0\n",
    "        # grade                          normal\n",
    "        # day                           weekend\n",
    "        # dateName                          NaN\n",
    "        # Name: 1, dtype: object\n",
    "# 이렇게 출력되는데, 아래 형식처럼 나오는 것이다.\n",
    "# 행1의 정보를 컬럼명1 값1 \\ 컬럼명2 값2 \\ ... \n",
    "# 행2의 정보를 컬럼명1 값1 \\ 컬럼명2 값2 \\ ...\n",
    "\n",
    "# date_name2 = merge_date.apply(lambda row : 'weekend' \n",
    "#                               if str(row['dateName']) != 'nan' else row['day'], axis=1)\n",
    "    # date_name2 시리즈를 생성\n",
    "    # 병합된 df에 apply를 lambda를 활용해서 적용\n",
    "    # 1. dateName컬럼을 문자열로 데이터 타입 바꾸기 (NaN > 'nan')\n",
    "    # 2. 행별로 dateName이 'nan'이 아니라면 'weekend' 값을 date_name2 시리즈에 넣기\n",
    "    # 3. 'nan'이라면 day컬럼의 값을 대입해서 date_name2 시리즈에 넣기\n",
    "\n",
    "# merge_date['day2'] = date_name2\n",
    "# 병합된 df에 day2라는 컬럼을 만들어서 위에서 만든 date_name2 시리즈의 값을 넣기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수를 이용해서 계절 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 계절 구분하기 (봄: 3~5, 여름: 6~8, 가을: 9~11, 겨울: 12~2)\n",
    "# def get_season(month):\n",
    "#     if month in [3, 4, 5]:\n",
    "#         return 'Spring'\n",
    "#     elif month in [6, 7, 8]:\n",
    "#         return 'Summer'\n",
    "#     elif month in [9, 10, 11]:\n",
    "#         return 'Fall'\n",
    "#     else:\n",
    "#         return 'Winter'\n",
    "\n",
    "# fine_dust['season'] = fine_dust['date'].dt.month.apply(get_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 등급 순서 정의\n",
    "# grade_categories = ['good', 'normal', 'bad', 'very_bad']  # 원하는 순서\n",
    "# grade_order = pd.CategoricalDtype(categories=grade_categories, ordered=True)\n",
    "# season_order = pd.CategoricalDtype(categories=['Spring', 'Summer', 'Fall', 'Winter'], ordered=True)\n",
    "\n",
    "# # 데이터프레임의 grade 열을 순서가 있는 카테고리형으로 변환\n",
    "# fine_dust['grade'] = fine_dust['grade'].astype(grade_order)\n",
    "# fine_dust['season'] = fine_dust['season'].astype(season_order)\n",
    "\n",
    "# # 데이터 그룹화\n",
    "# grouped_data = fine_dust.groupby(['year', 'season', 'grade']).size().unstack(fill_value=0)\n",
    "\n",
    "# # 원하는 순서대로 컬럼 재정렬\n",
    "# grouped_data = grouped_data[grade_categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "범주화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서가 있는 범주형 지정 방법\n",
    "# from pandas.api.types import CategoricalDtype\n",
    "# categori1 = ['값1', '값2', '값3']     # 이때, 범주의 순서가 값1 > 값2 > 값3으로 지정된다\n",
    "# c_d1 = CategoricalDtype(categories = categori1, ordered = True)\n",
    "# df[\"범주1을 지정하고싶은 컬럼이름\"] = df[\"범주1을 지정하고싶은 컬럼이름\"].astype(c_d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그룹화 예시\n",
    "# df.groupby('class') # class컬럼을 기준으로 그룹화\n",
    "# df.groupby('class').count() # class 컬럼을 기준으로 그룹화한 뒤 타 컬럼별로 not null 개수 세기\n",
    "\n",
    "# group_ex = df.groupby('class').count()\n",
    "# group_ex.loc[1, 'name'] # 1반의 'name' column 유효 element 개수, 4가 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 그룹화\n",
    "# # grouped_data = fine_dust.groupby(['year', 'season', 'grade']).size().unstack(fill_value=0)\n",
    "# 윗 줄 코드는 from GPT\n",
    "\n",
    "# # 단일 컬럼을 대상으로 그룹\n",
    "# m_fine_dust.groupby('merge_day') # 평일/주말(+공휴일) 두 그룹으로 나누기\n",
    "# m_fine_dust.groupby('merge_day').count()  # day그룹을 기준으로 not null 값 개수 세기\n",
    "\n",
    "# gr = m_fine_dust.groupby('merge_day').count()\n",
    "# gr.loc['weekday', 'grade']\n",
    "# # 평일/주말 두 그룹을 만들어서 gr에 저장 후, 그룹 weekday(평일)의 'grade'column 유효 개수 세기 # 902개\n",
    "\n",
    "# m_fine_dust.groupby('grade')['fine_dust(㎍/㎥)'].mean()\n",
    "# # grade컬럼의 4개 값을 기준으로 그룹한 뒤, 각 그룹의 미세먼지 농도 평균 계산\n",
    "\n",
    "# # 다중 컬럼을 대상으로 그룹\n",
    "# m_fine_dust.groupby(['merge_day', 'grade']).count()\n",
    "# # 평일/주말을 대상으로 그룹한 뒤, 각 그룹 안에서 등급별로 4개를 다시 그룹 > 총 8개 그룹 탄생\n",
    "# # 그 8개 그룹을 대상으로 유효한 값(not null) 개수 세기\n",
    "\n",
    "# pd.DataFrame(m_fine_dust.groupby(['merge_day', 'grade'])['fine_dust(㎍/㎥)'].mean()). \\\n",
    "#                                         sort_values(by = 'fine_dust(㎍/㎥)', ascending = False)\n",
    "# # 평일/주말 그룹 후, 각 그룹 안에서 등급별로 재그룹한 뒤, 각 그룹의 농도 컬럼 값 평균내기\n",
    "# # 평균계산 된 농도 컬럼 값을 기준으로 정렬하기 (ascending = False 내림차순)\n",
    "\n",
    "# 예시\n",
    "# df.groupby(['나이', '학과']).count().unstack(fill_value=0).stack()\n",
    "# 나이별 그룹 > 학과별 그룹 \n",
    "# > 해당 그룹 안에서 컬럼의 값들이 없는 경우는 그룹화를 풀어서 0으로 채운후 재그룹화\n",
    "\n",
    "# 그룹화 .count()와 .size()의 차이\n",
    "# count() - 함수로써 NaN을 제외하고 값을 계산\n",
    "# size() - 데이터프레임의 속성으로써, NaN을 포함하고 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 개별 그룹화 및 분석\n",
    "# gr_season = m_fine_dust.groupby(\"season\")[\"fine_dust(㎍/㎥)\"].mean()\n",
    "# gr_grade = m_fine_dust.groupby(\"grade\")[\"fine_dust(㎍/㎥)\"].count()\n",
    "\n",
    "# # 그룹 풀기\n",
    "# season_reset = gr_season.reset_index()  # 시즌별 분석 결과가 데이터프레임으로 변환\n",
    "# grade_reset = gr_grade.reset_index()    # 등급별 분석 결과가 데이터프레임으로 변환\n",
    "\n",
    "# # 다시 원하는 방식으로 그룹화 가능\n",
    "# 예시: 계절별 평균과 등급 분포를 한 번에 보기\n",
    "# season_analysis = m_fine_dust.groupby(\"season\").agg({\n",
    "#     \"fine_dust(㎍/㎥)\": \"mean\",\n",
    "#     \"grade\": \"value_counts\"\n",
    "# })\n",
    "\n",
    "# # 예시: 계절별, 요일별 평균과 개수\n",
    "# season_day_analysis = m_fine_dust.groupby([\"season\", \"merge_day\"]).agg({\n",
    "#     \"fine_dust(㎍/㎥)\": [\"mean\", \"count\"]\n",
    "# })\n",
    "\n",
    "# # 그룹 결과를 데이터프레임으로 변환\n",
    "# result = m_fine_dust.groupby(\"season\")[\"fine_dust(㎍/㎥)\"].mean().reset_index()\n",
    "\n",
    "# # 이제 이 결과로 새로운 분석 가능\n",
    "# new_analysis = result.groupby(...).mean()  # 새로운 그룹화 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 그룹화\n",
    "# # 평일 / 주말 및 공휴일\n",
    "# observed=True를 붙여야 에러 메세지 안 나옴\n",
    "# gr_year = m_fine_dust.groupby(\"year\", observed=True).count()       # 연도별 그룹화\n",
    "# gr_grade = m_fine_dust.groupby(\"grade\", observed=True).count()     # 등급별 그룹화\n",
    "# gr_season = m_fine_dust.groupby(\"season\", observed=True).count()   # 계절별 그룹화\n",
    "# gr_day = m_fine_dust.groupby(\"merge_day\", observed=True).count()   # 평일/주말별 그룹화\n",
    "\n",
    "# 그룹화한 다음 그룹의 계산결과를 DF로 불러와야 reset_index()로 그룹해제 할 수 있다\n",
    "\n",
    "# # 그룹 풀기\n",
    "# grade_reset = gr_grade.reset_index()    # 등급별 분석 결과가 데이터프레임으로 변환\n",
    "# year_reset = gr_year.reset_index()      # 연도별 분석 결과가 데이터프레임으로 변환\n",
    "# season_reset = gr_season.reset_index()  # 계절별 분석 결과가 데이터프레임으로 변환\n",
    "# day_reset = gr_day.reset_index()        # 평/주말별 분석 결과가 데이터프레임으로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미세먼지 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석 방향\n",
    "\n",
    "# 1. 단일 그룹 분석 (1-1 ~ 1-4)\n",
    "    # 단일 변수에 따른 미세먼지 농도의 변화를 확인하는 기본 \n",
    "    # 평균, 분산, 최대/최소 값, 빈도 등으로 미세먼지 수준의 분포를 파악\n",
    "\n",
    "# 연도별 미세먼지 분석 (1-1)\n",
    "    # 연도별 평균 미세먼지 농도, 최대/최소 농도 계산.\n",
    "    # 연도별로 \"좋음\"부터 \"매우 나쁨\" 등급까지의 빈도 비율.\n",
    "    # 연도별 농도 변화 추이 시각화 (예: 선 그래프).\n",
    "\n",
    "# 등급별 미세먼지 분석 (1-2)\n",
    "    # 각 등급의 평균 농도, 등급별 농도 분포.\n",
    "    # 각 등급의 일수 또는 빈도 비율.\n",
    "    # 등급별로 평균 및 분산 계산 후 Box plot으로 분포 확인.\n",
    "\n",
    "# 계절별 미세먼지 분석 (1-3)\n",
    "    # 계절별 평균 및 분산, 최대/최소 농도 확인.\n",
    "    # 계절별 \"좋음\"에서 \"매우 나쁨\" 등급까지의 비율 비교.\n",
    "    # 계절에 따른 변화 패턴을 선 그래프로 시각화하여 특정 계절의 농도 피크 확인.\n",
    "\n",
    "# 평일/주말 미세먼지 분석 (1-4)\n",
    "    # 평일과 주말의 평균 농도, 각 등급 빈도 비교.\n",
    "    # Box plot으로 평일/주말 농도의 분포 차이 확인.\n",
    "\n",
    "# 2. 2개 변수 조합 분석 (2-1 ~ 2-6)\n",
    "    # 두 변수의 상호작용에 따른 미세먼지 농도 패턴 찾기\n",
    "    # 이때 그룹별 평균, 빈도, 분포 변화 확인\n",
    "\n",
    "# 연도와 등급 (2-1)\n",
    "    # 각 연도별 등급별 평균 농도 및 빈도 변화.\n",
    "    # 특정 등급의 연도별 변화 추세 (예: \"매우 나쁨\" 비율이 증가했는지 여부).\n",
    "\n",
    "# 연도와 계절 (2-2)\n",
    "    # 연도 및 계절별 평균 농도, 분산 확인.\n",
    "    # 계절별 연도 변화 추세 확인 (예: 봄철 미세먼지 농도가 해마다 증가하는지 확인).\n",
    "\n",
    "# 연도와 평일/주말 (2-3)\n",
    "    # 연도별로 평일/주말의 농도 평균 차이 분석.\n",
    "    # 평일/주말 패턴이 연도에 따라 어떻게 달라지는지 추세 확인.\n",
    "\n",
    "# 등급과 계절 (2-4)\n",
    "    # 각 계절에서 등급별 빈도 확인 (예: 봄철에 \"나쁨\" 빈도가 높은지).\n",
    "    # 계절에 따라 특정 등급의 농도가 차이 나는지 확인.\n",
    "\n",
    "# 등급과 평일/주말 (2-5)\n",
    "    # 평일/주말로 등급별 빈도 분석하여 특정 등급이 주말에 더 많은지 확인.\n",
    "    # 등급별 평일과 주말의 평균 농도 차이 확인.\n",
    "\n",
    "# 계절과 평일/주말 (2-6)\n",
    "    # 계절별 평일/주말에 따른 농도 차이 분석.\n",
    "    # 특정 계절에 주말에 더 높은 농도를 보이는지 확인.\n",
    "\n",
    "# 3. 3개 변수 조합 분석 (3-1 ~ 3-3)\n",
    "    # 세 변수의 조합으로 보다 세밀하게 미세먼지 농도 패턴을 파악\n",
    "\n",
    "# 연도, 등급, 계절 (3-1)\n",
    "    # 연도별로 계절과 등급의 빈도를 분석\n",
    "    # 특정 연도의 특정 계절에 특정 등급이 빈번하게 나타나는지 확인.\n",
    "\n",
    "# 연도, 등급, 평일/주말 (3-2)\n",
    "    # 연도별로 평일/주말에 따라 등급이 어떻게 분포하는지 분석.\n",
    "\n",
    "# 등급, 계절, 평일/주말 (3-3)\n",
    "    # 각 계절에서 평일과 주말의 등급 분포 차이 확인.\n",
    "\n",
    "# 4. 4개 변수 조합 분석 (4-1)\n",
    "    # 모든 변수를 고려하여 최종적으로 미세먼지 농도의 패턴을 분석\n",
    "\n",
    "# 연도, 등급, 계절, 평일/주말 (4-1)\n",
    "    # 각 연도, 계절, 평일/주말, 등급에 따른 농도 분포를 분석하여 \n",
    "    # 특정 조건에서 높은 농도를 보이는 패턴을 파악.\n",
    "    # 히트맵 또는 피벗 테이블을 통해 전체 패턴을 시각화하여 \n",
    "    # 특정 조건에서의 농도 분포 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 분석 방법과 예시\n",
    "\n",
    "# 평균 : .mean()\n",
    "        # # 등급별 미세먼지 농도 평균 계산\n",
    "        # gr_grade_mean = gr_grade[\"fine_dust(㎍/㎥)\"].mean()\n",
    "        # print(gr_grade_mean)\n",
    "\n",
    "# 최대 : .max()\n",
    "        # # 등급별 미세먼지 농도 최대 값\n",
    "        # gr_grade_max = gr_grade[\"fine_dust(㎍/㎥)\"].max()\n",
    "        # print(gr_grade_max)\n",
    "\n",
    "# 최소 : .min()\n",
    "        # # 등급별 미세먼지 농도 최대 값\n",
    "        # gr_grade_min = gr_grade[\"fine_dust(㎍/㎥)\"].min()\n",
    "        # print(gr_grade_min)\n",
    "\n",
    "# 분산 : .var()\n",
    "        # # 등급별 미세먼지 농도 분산 계산\n",
    "        # gr_grade_variance = gr_grade[\"fine_dust(㎍/㎥)\"].var()\n",
    "        # print(gr_grade_variance)\n",
    "      \n",
    "# 빈도 : .size() or count()\n",
    "        # # 등급별 빈도 계산\n",
    "        # gr_grade_frequency = gr_grade.size()\n",
    "        # print(gr_grade_frequency)\n",
    "        # # 특정 컬럼에 대한 등급별 빈도 계산\n",
    "        # gr_grade_count = gr_grade[\"fine_dust(㎍/㎥)\"].count()\n",
    "        # print(gr_grade_count)\n",
    "\n",
    "# 분포변화 : 시각화 - 히스토그램 및 KDE(커널 밀도 추정) 그래프\n",
    "        # 히스토그램\n",
    "        # import matplotlib.pyplot as plt\n",
    "\n",
    "        # # 등급별 미세먼지 농도 분포 시각화\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # for grade in m_fine_dust[\"grade\"].unique():\n",
    "        #     subset = m_fine_dust[m_fine_dust[\"grade\"] == grade]\n",
    "        #     plt.hist(subset[\"fine_dust(㎍/㎥)\"], bins=20, alpha=0.5, label=grade)\n",
    "\n",
    "        # plt.title(\"Distribution of Fine Dust Concentration by Grade\")\n",
    "        # plt.xlabel(\"Fine Dust Concentration (㎍/㎥)\")\n",
    "        # plt.ylabel(\"Frequency\")\n",
    "        # plt.legend()\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석 양식:\n",
    "# 1-1.  분석\n",
    "    # 그룹화\n",
    "    # 별 최대 값 계산\n",
    "    # 별 최소 값 계산\n",
    "    # 별 평균 계산\n",
    "    # 별 분산 계산\n",
    "    # 별 빈도 계산\n",
    "    # 그룹해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1-1. 연도별 미세먼지 분석\n",
    "\n",
    "# # 연도별 그룹화\n",
    "# gr_year = m_fine_dust.groupby(\"year\", observed=True)\n",
    "\n",
    "# # 연도별 최대 값 계산\n",
    "# gr_year_max = gr_year[\"fine_dust(㎍/㎥)\"].max()\n",
    "\n",
    "# # 연도별 최소 값 계산\n",
    "# gr_year_min = gr_year[\"fine_dust(㎍/㎥)\"].min()\n",
    "\n",
    "# # 연도별 평균 계산\n",
    "# gr_year_mea = gr_year[\"fine_dust(㎍/㎥)\"].mean()\n",
    "\n",
    "# # 연도별 분산 계산\n",
    "# gr_year_var = gr_year[\"fine_dust(㎍/㎥)\"].var()\n",
    "\n",
    "# # 연도별 빈도 계산\n",
    "# gr_year_cou = gr_year[\"fine_dust(㎍/㎥)\"].count()\n",
    "\n",
    "# print( \"\\n연도별 최대 값 : \\n\", gr_year_max)\n",
    "# print( \"\\n연도별 최소 값 : \\n\", gr_year_min)\n",
    "# print( \"\\n연도별 평균 : \\n\", gr_year_mea)\n",
    "# print( \"\\n연도별 분산 : \\n\", gr_year_var)\n",
    "# print( \"\\n연도별 빈도 : \\n\", gr_year_cou )\n",
    "\n",
    "# # 그룹화 해제\n",
    "# gr_year_max_reset = gr_year_max.reset_index()\n",
    "# gr_year_min_reset = gr_year_min.reset_index()\n",
    "# gr_year_mea_reset = gr_year_mea.reset_index()\n",
    "# gr_year_var_reset = gr_year_var.reset_index()\n",
    "# gr_year_cou_reset = gr_year_cou.reset_index()\n",
    "\n",
    "# 이렇게 길었던 코드가 (심지어 그룹별로 따로 해야함)\n",
    "# def group_analysis_by_single_column(group_col_name, col_name):\n",
    "#     # 그룹화\n",
    "#     grouped_df = m_fine_dust.groupby(group_col_name, observed=True)\n",
    "\n",
    "#     # 그룹별 최대/최소/평균/분산/빈도 계산\n",
    "#     stats = grouped_df[col_name].agg([\"max\", \"min\", \"mean\", \"var\", \"count\"])\n",
    "#     return stats\n",
    "\n",
    "# print( group_analysis_by_single_column(\"year\", \"fine_dust(㎍/㎥)\") )         # 연도별    \n",
    "# print( group_analysis_by_single_column(\"grade\", \"fine_dust(㎍/㎥)\") )        # 등급별    \n",
    "# print( group_analysis_by_single_column(\"season\", \"fine_dust(㎍/㎥)\") )       # 계절별    \n",
    "# print( group_analysis_by_single_column(\"merge_day\", \"fine_dust(㎍/㎥)\") )    # 평일/주말별    \n",
    "\n",
    "# 함수를 써서 이렇게 간소화됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def group_analysis_by_single_column(group_col_name, col_name):\n",
    "# # 해석 : group_analysis_by_single_column 이라는 변수를 만들고, \n",
    "# # 전달인자로 group_col_name과 col_name을 만듦\n",
    "\n",
    "#     # 그룹화\n",
    "#     grouped_df = m_fine_dust.groupby(group_col_name, observed=True)\n",
    "# # 해석 : grouped_df라는 변수를 만들고 m_fine_dust라는 데이터 프레임에서, \n",
    "# # group_col_name이라는 전달인자를 받으면 그룹화하기(observed=True로 지정해서 에러 메세지 없애기)\n",
    "\n",
    "#     # 그룹별 최대/최소/평균/분산/빈도 계산\n",
    "#     stats = grouped_df[col_name].agg([\"max\", \"min\", \"mean\", \"var\", \"count\"])\n",
    "#     return stats\n",
    "# # 해석 : stats라는 변수를 만들어서, \n",
    "# # 위에서 만든 grouped_df(단일 컬럼을 그룹화한 데이터프레임의 변수명)에 \n",
    "# # 분석하고자하는 컬럼을 col_name 이라는 전달인자로 전달받아서 \n",
    "# # 최대, 최소, 평균, 분산, 빈도를 agg로 한꺼번에 계산하여 저장하기\n",
    "\n",
    "# # 단일 그룹 분석 결과를 데이터 프레임으로 저장\n",
    "# df_year_stats = group_analysis_by_single_column(\"year\", \"fine_dust(㎍/㎥)\")\n",
    "# df_grade_stats = group_analysis_by_single_column(\"grade\", \"fine_dust(㎍/㎥)\")\n",
    "# df_season_stats = group_analysis_by_single_column(\"season\", \"fine_dust(㎍/㎥)\")\n",
    "# df_day_stats = group_analysis_by_single_column(\"merge_day\", \"fine_dust(㎍/㎥)\")\n",
    "# # 해석 : 위의 그룹화화 분석을 위한 함수로 출력된 결과를 저장하는 데이터 프레임을 그룹별로 만들기. \n",
    "# # 이때, 함수에서 정의한 변수와 같은 이름으로 만들어서 ()안의 내용이 전달인자의 내용임. \n",
    "# # 그래서 이 경우, 예를 들어 (\"year\", \"fine_dust(㎍/㎥)\")라고 써도 되지만, \n",
    "# # (group_col_name = \"year\", col_name = \"fine_dust(㎍/㎥)\")이라고 코드를 써서 \n",
    "# # 전달인자의 가독성을 높일 수도 있음\n",
    "\n",
    "# # 데이터 프레임을 하나로 합치기\n",
    "# combined_stats = {\n",
    "#     \"Year\": df_year_stats,\n",
    "#     \"Grade\": df_grade_stats,\n",
    "#     \"Season\": df_season_stats,\n",
    "#     \"Day Type\": df_day_stats\n",
    "# }\n",
    "# # 해석 : 함수로 나온 각 그룹별 통계치를 한꺼번에 저장하는 딕셔너리인 combined_stats를 만들어서\n",
    "# # 각 그룹별 통계치를 \"name\" : df_name, 형태로 만들어서, \n",
    "# # 마치 combined_stats라는 딕셔너리 안에 각 그룹별 데이터 프레임이 들어간 형태를 취함\n",
    "\n",
    "# print( \"단일 그룹 분석\")\n",
    "# # 결과 확인\n",
    "# for key, value in combined_stats.items():\n",
    "#     print(f\"\\n\\n{key} Stats:\\n\", value)\n",
    "# # 해석 : 위에서 합친 combined_stats가 딕셔너리 형태이므로, \n",
    "# # 각 키인 'Year', 'Grade', 'Season', 'Day Type'의 value인 통계치 값을 출력하기. \n",
    "# # 이때 (f\"\\n\\n{key} Stats:\\n\", value)형태를 취해서 각 키별 밸류가 출력됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 : 이때 .items()가 뭔지 모르겠음\n",
    "# 해답: \n",
    "# .keys(): 키만 가져올 때\n",
    "# .values(): 값만 가져올 때\n",
    "# 키와 값을 동시에 처리할 때는 .items()가 가장 편리\n",
    "\n",
    "# # 간단한 딕셔너리 예시\n",
    "# fruit_prices = {\n",
    "#     \"apple\": 1000,\n",
    "#     \"banana\": 1500,\n",
    "#     \"orange\": 2000\n",
    "# }\n",
    "\n",
    "# # .items() 사용하지 않고 출력하면\n",
    "# print(\"1. 그냥 딕셔너리 출력:\")\n",
    "# print(fruit_prices)  # {'apple': 1000, 'banana': 1500, 'orange': 2000}\n",
    "\n",
    "# # .items() 사용하면\n",
    "# print(\"\\n2. .items()로 출력:\")\n",
    "# print(fruit_prices.items())  # dict_items([('apple', 1000), ('banana', 1500), ('orange', 2000)])\n",
    "\n",
    "# # for 루프에서 .items() 사용\n",
    "# print(\"\\n3. for 루프로 각각 출력:\")\n",
    "# for key, value in fruit_prices.items():\n",
    "#     print(f\"과일: {key}, 가격: {value}원\")\n",
    "# # 출력:\n",
    "# # 과일: apple, 가격: 1000원\n",
    "# # 과일: banana, 가격: 1500원\n",
    "# # 과일: orange, 가격: 2000원\n",
    "\n",
    "# # 위의 combined_stats와 비슷한 구조로 예시\n",
    "# stats_example = {\n",
    "#     \"Year\": \"연도별 통계\",\n",
    "#     \"Grade\": \"등급별 통계\"\n",
    "# }\n",
    "\n",
    "# print(\"\\n4. stats 출력 예시:\")\n",
    "# for key, value in stats_example.items():\n",
    "#     print(f\"\\n{key} Stats:\\n\", value)\n",
    "# # 출력:\n",
    "# # Year Stats:\n",
    "# # 연도별 통계\n",
    "# # Grade Stats:\n",
    "# # 등급별 통계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단일분석 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이게 최초 코드.\n",
    "# 문제 발생 -> 분산만 너무 큼 -> 수정 필요 \n",
    "\n",
    "# # 연도별 통계 시각화\n",
    "# df_year_stats.plot(kind='bar', figsize=(10, 6), title='Yearly Fine Dust Statistics', \n",
    "#                    color=['skyblue', 'salmon', 'lightgreen', 'orange', 'lightcoral'])\n",
    "# plt.ylabel('Values')\n",
    "# plt.xlabel('Year')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(title='Statistics')\n",
    "# plt.grid(axis='y')\n",
    "# plt.show()\n",
    "\n",
    "# # 등급별 통계 시각화\n",
    "# df_grade_stats.plot(kind='bar', figsize=(10, 6), title='Grade-wise Fine Dust Statistics', \n",
    "#                     color=['skyblue', 'salmon', 'lightgreen', 'orange', 'lightcoral'])\n",
    "# plt.ylabel('Values')\n",
    "# plt.xlabel('Grade')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(title='Statistics')\n",
    "# plt.grid(axis='y')\n",
    "# plt.show()\n",
    "\n",
    "# # 계절별 통계 시각화\n",
    "# df_season_stats.plot(kind='bar', figsize=(10, 6), title='Seasonal Fine Dust Statistics', \n",
    "#                      color=['skyblue', 'salmon', 'lightgreen', 'orange', 'lightcoral'])\n",
    "# plt.ylabel('Values')\n",
    "# plt.xlabel('Season')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(title='Statistics')\n",
    "# plt.grid(axis='y')\n",
    "# plt.show()\n",
    "\n",
    "# # 평일/주말별 통계 시각화\n",
    "# df_day_stats.plot(kind='bar', figsize=(10, 6), title='Weekday/Weekend Fine Dust Statistics', \n",
    "#                   color=['skyblue', 'salmon', 'lightgreen', 'orange', 'lightcoral'])\n",
    "# plt.ylabel('Values')\n",
    "# plt.xlabel('Day Type')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(title='Statistics')\n",
    "# plt.grid(axis='y')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_stats_with_count(stats_df, ax, title):\n",
    "#     \"\"\"\n",
    "#     Parameters:\n",
    "#         stats_df: 통계치를 포함한 데이터 프레임\n",
    "#         ax: 서브플롯의 축\n",
    "#         title: 그래프 제목\n",
    "#     \"\"\"\n",
    "#     # 첫 번째 y축에 max, min, mean 플롯\n",
    "#     stats_df[['max', 'min', 'mean']].plot(kind='bar', ax=ax, \n",
    "#                                          color=['skyblue', 'salmon', 'lightgreen'],\n",
    "#                                          width=0.8)\n",
    "    \n",
    "#     # 두 번째 y축 생성\n",
    "#     ax2 = ax.twinx()\n",
    "    \n",
    "#     # count를 선 그래프로 표시\n",
    "#     count_line = ax2.plot(range(len(stats_df)), stats_df['count'], \n",
    "#                          color='sandybrown', marker='o', linewidth=2, \n",
    "#                          label='count')\n",
    "    \n",
    "#     # 첫 번째 축 설정\n",
    "#     ax.set_title(title)\n",
    "#     ax.set_xlabel(stats_df.index.name)\n",
    "#     ax.set_ylabel('Values')\n",
    "#     ax.tick_params(axis='x', rotation=45)\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # 두 번째 축 설정\n",
    "#     ax2.set_ylabel('Count')\n",
    "#     ax2.tick_params(axis='y', labelcolor='sandybrown')\n",
    "    \n",
    "#     # 범례 통합\n",
    "#     lines1, labels1 = ax.get_legend_handles_labels()\n",
    "#     lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "#     ax2.legend(lines1 + count_line, labels1 + ['count'], \n",
    "#               loc='upper right')\n",
    "\n",
    "# # 서브플롯 생성\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# # 각 서브플롯에 그래프 그리기\n",
    "# plot_stats_with_count(df_year_stats, axs[0, 0], \n",
    "#                      'Yearly Fine Dust Statistics')\n",
    "# plot_stats_with_count(df_grade_stats, axs[0, 1], \n",
    "#                      'Grade-wise Fine Dust Statistics')\n",
    "# plot_stats_with_count(df_season_stats, axs[1, 0], \n",
    "#                      'Seasonal Fine Dust Statistics')\n",
    "# plot_stats_with_count(df_day_stats, axs[1, 1], \n",
    "#                      'Weekday/Weekend Fine Dust Statistics')\n",
    "\n",
    "# # 레이아웃 조정\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단일분석 - 시각화 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 그래프별로 데이터 분석\n",
    "\n",
    "# Yearly Fine Dust Statistics (연도별 미세먼지 통계):\n",
    "# 2021년이 가장 높은 최대값(약 380)\n",
    "# 2022-2023년은 최대값이 감소하는 추세\n",
    "# 2024년(1월-10월30일)은 다른 연도보다 데이터 수(count)가 적음\n",
    "# 전반적으로 평균값은 50 미만으로 유지됨\n",
    "\n",
    "# Grade-wise Fine Dust Statistics (등급별 미세먼지 통계):\n",
    "# very_bad(매우 나쁨) 등급이 가장 높은 최대값(약 380)\n",
    "# 등급이 나빠질수록(good→very_bad) 최대값, 최소값, 평균값 모두 증가\n",
    "# good(좋음)과 normal(보통)은 상대적으로 많은 발생 빈도\n",
    "# very_bad는 발생 빈도가 가장 낮음\n",
    "\n",
    "# Seasonal Fine Dust Statistics (계절별 미세먼지 통계):\n",
    "# 봄(Spring)에 가장 높은 최대값\n",
    "# 여름(Summer)과 가을(Fall)은 상대적으로 낮은 수치\n",
    "# 봄과 여름이 다른 계절보다 발생 빈도가 높음\n",
    "# 겨울(Winter)은 중간 정도의 수치를 보임\n",
    "\n",
    "# Weekday/Weekend Fine Dust Statistics (평일/주말 미세먼지 통계):\n",
    "# 평일(weekday)과 주말(weekend)의 최대값이 비슷\n",
    "# 평일이 주말보다 약간 더 많은 발생 빈도\n",
    "# 평균값은 평일과 주말이 비슷한 수준(약 40)\n",
    "\n",
    "# 전반적인 특징:\n",
    "# 계절성: 봄철에 미세먼지가 가장 심각\n",
    "# 연도별 개선: 2021년 이후 최대값이 감소하는 추세\n",
    "# 발생 빈도: 좋음/보통 등급이 가장 많이 발생\n",
    "# 평일/주말 차이: 큰 차이는 없으나 평일이 조금 더 빈번\n",
    "\n",
    "# 미세먼지 관리나 대책 수립 시 봄철에 더 집중적인 관리가 필요하며, \n",
    "# 전반적으로 미세먼지 상황이 개선되고 있다는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_stats_with_count(stats_df, ax, title):\n",
    "#     \"\"\"\n",
    "#     Parameters:\n",
    "#         stats_df: 통계치를 포함한 데이터 프레임\n",
    "#         ax: 서브플롯의 축\n",
    "#         title: 그래프 제목\n",
    "#     \"\"\"\n",
    "#     # 막대 그래프 생성\n",
    "#     bars = stats_df[['max', 'min', 'mean']].plot(kind='bar', ax=ax, \n",
    "#                                                 color=['skyblue', 'salmon', 'lightgreen'],\n",
    "#                                                 width=0.8)\n",
    "    \n",
    "#     # 2024년 막대에 패턴 추가\n",
    "#     if 'year' in str(stats_df.index.name).lower():  # year 관련 그래프인 경우에만\n",
    "#         for container in bars.containers:  # max, min, mean 각각의 막대에 대해\n",
    "#             # 2024년에 해당하는 막대에 패턴 추가\n",
    "#             for idx, patch in enumerate(container):\n",
    "#                 if str(stats_df.index[idx]) == '2024':\n",
    "#                     patch.set_hatch('///')\n",
    "#                     patch.set_alpha(0.8)\n",
    "    \n",
    "#     # 두 번째 y축 생성\n",
    "#     ax2 = ax.twinx()\n",
    "    \n",
    "#     # count를 선 그래프로 표시\n",
    "#     count_line = ax2.plot(range(len(stats_df)), stats_df['count'], \n",
    "#                          color='sandybrown', marker='o', linewidth=2, \n",
    "#                          label='count')\n",
    "    \n",
    "#     # 첫 번째 축 설정\n",
    "#     ax.set_title(title)\n",
    "#     ax.set_xlabel(stats_df.index.name)\n",
    "#     ax.set_ylabel('Values')\n",
    "#     ax.tick_params(axis='x', rotation=45)\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # 두 번째 축 설정\n",
    "#     ax2.set_ylabel('Count')\n",
    "#     ax2.tick_params(axis='y', labelcolor='sandybrown')\n",
    "    \n",
    "#     # 범례 통합\n",
    "#     lines1, labels1 = ax.get_legend_handles_labels()\n",
    "#     lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "#     ax2.legend(lines1 + count_line, labels1 + ['count'], \n",
    "#               loc='upper right')\n",
    "    \n",
    "#     # 2024년 데이터가 부분적임을 설명하는 주석 추가\n",
    "#     if 'year' in str(stats_df.index.name).lower():\n",
    "#         ax.annotate('* 2024 data: Jan-Oct 30th', \n",
    "#                    xy=(0.02, 0.98), \n",
    "#                    xycoords='axes fraction',\n",
    "#                    bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8),\n",
    "#                    fontsize=8,\n",
    "#                    va='top')\n",
    "\n",
    "# # 서브플롯 생성\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# # 각 서브플롯에 그래프 그리기\n",
    "# plot_stats_with_count(df_year_stats, axs[0, 0], \n",
    "#                      'Yearly Fine Dust Statistics')\n",
    "# plot_stats_with_count(df_grade_stats, axs[0, 1], \n",
    "#                      'Grade-wise Fine Dust Statistics')\n",
    "# plot_stats_with_count(df_season_stats, axs[1, 0], \n",
    "#                      'Seasonal Fine Dust Statistics')\n",
    "# plot_stats_with_count(df_day_stats, axs[1, 1], \n",
    "#                      'Weekday/Weekend Fine Dust Statistics')\n",
    "\n",
    "# # 레이아웃 조정\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 그래프를 파일로 저장\n",
    "# plt.savefig('./data/Single_Group_Analysis_Plots.png', \n",
    "#             dpi=300,  # 해상도 설정\n",
    "#             bbox_inches='tight',  # 여백 자동 조정\n",
    "#             pad_inches=0.1)  # 여백 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 정규화해서 스케일 맞추기 - 고만고만해져서 비교가 어려움\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# def plot_norm_stats(df, title, ylabel, figsize=(10, 6)):\n",
    "#     \"\"\"\n",
    "#     Parameters:\n",
    "#         df: 통계치를 포함한 데이터 프레임\n",
    "#         title: 그래프 제목\n",
    "#         ylabel: y축 레이블\n",
    "#         figsize: 그래프 크기\n",
    "#     \"\"\"\n",
    "#     # 정규화\n",
    "#     norm = MinMaxScaler().fit_transform(df)\n",
    "\n",
    "#     # 정규화된 데이터 프레임으로 변환\n",
    "#     norm_df = pd.DataFrame(norm, columns=df.columns, index=df.index)\n",
    "\n",
    "#     # 정규화된 데이터 시각화\n",
    "#     norm_df.plot(kind='bar', figsize=figsize, title=title, \n",
    "#                  color=['skyblue', 'salmon', 'lightgreen', 'orange'])\n",
    "#     plt.ylabel(ylabel)\n",
    "#     plt.xlabel(df.index.name)\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.legend(title='Statistics')\n",
    "#     plt.grid(axis='y')\n",
    "#     plt.show()\n",
    "\n",
    "# # 연도별 통계 시각화 (정규화)\n",
    "# plot_norm_stats(df_year_stats, \n",
    "#                 'Normalized Yearly Fine Dust Stats', 'Normalized Values')\n",
    "\n",
    "# # 등급별 통계 시각화 (정규화)\n",
    "# plot_norm_stats(df_grade_stats, \n",
    "#                 'Normalized Grade-wise Fine Dust Stats', 'Normalized Values')\n",
    "\n",
    "# # 계절별 통계 시각화 (정규화)\n",
    "# plot_norm_stats(df_season_stats, \n",
    "#                 'Normalized Seasonal Fine Dust Stats', 'Normalized Values')\n",
    "\n",
    "# # 평일/주말별 통계 시각화 (정규화)\n",
    "# plot_norm_stats(df_day_stats, \n",
    "#                 'Normalized Weekday/Weekend Fine Dust Stats', 'Normalized Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개 변수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def group_analysis_by_two_columns(group_col1, group_col2, target_col):\n",
    "# 해석 : \n",
    "# 함수 사용. 함수의 변수명은 group_analysis_by_two_columns\n",
    "# 전달인자로 group_col1, group_col2, target_col 3개를 만듦\n",
    "\n",
    "#     # 두 컬럼으로 그룹화\n",
    "#     grouped_df = m_fine_dust.groupby([group_col1, group_col2], observed=True)\n",
    "# 해석 : \n",
    "# grouped_df라는 데이터프레임을 생성\n",
    "# 해당 프레임에 m_fine_dust 데이터 프레임에서 2개 컬럼을 추출\n",
    "# 추출된 2개 컬럼을 그룹화하여 grouped_df에 저장하기\n",
    "# 이때 그룹화를 위한 컬럼 2개는 전달인자 group_col1, group_col2로 받기\n",
    "\n",
    "#     # 통계 계산\n",
    "#     stats = grouped_df[target_col].agg([\"max\", \"min\", \"mean\", \"var\", \"count\",\n",
    "#         \"median\",                              # 중앙값\n",
    "#         lambda x: x.quantile(0.25),            # 1사분위수 (Q1)\n",
    "#         lambda x: x.quantile(0.75),            # 3사분위수 (Q3)\n",
    "#         lambda x: x.quantile(0.75) - x.quantile(0.25), # IQR\n",
    "#         lambda x: skew(x),                     # 왜도\n",
    "#         lambda x: kurtosis(x),                 # 첨도\n",
    "#         lambda x: x.max() - x.min()            # 범위\n",
    "#     ])\n",
    "# 해석 : (lambda 계산 전)\n",
    "# stats라는 데이터 프레임을 생성\n",
    "# 위에서 만든 grouped_df에 저장된 그룹화된 2개 컬럼을 \n",
    "# agg를 사용하여 최대, 최소, 평균, 분산, 빈도를 한꺼번에 계산하기\n",
    "# 이 계산의 타겟은 target_col이라는 전달인자로 전달 받기\n",
    "\n",
    "# 해석 : (lambda 계산 이 후)\n",
    "# lambda 함수는 변수(v)로 전달된 데이터(여기서는 그룹별 미세먼지 농도 값들)를 입력으로 받아\n",
    "# 특정 통계값을 계산하는 간단한 함수로 작동\n",
    "#     lambda v: v.quantile(0.25)\n",
    "#         의미: 그룹별 데이터의 1사분위수(Q1, 25번째 백분위수)를 계산\n",
    "#         동작: v라는 데이터를 받아 v.quantile(0.25)로 1사분위수를 계산\n",
    "#     lambda v: v.quantile(0.75)  \n",
    "#         의미: 그룹별 데이터의 3사분위수(Q3, 75번째 백분위수)를 계산\n",
    "#         동작: v에 대해 v.quantile(0.75)로 3사분위수를 계산\n",
    "#     lambda v: v.quantile(0.75) - v.quantile(0.25)\n",
    "#         의미: IQR(Interquartile Range, 사분위 범위)을 계산합니다. IQR은 Q3에서 Q1을 뺀 값으로,\n",
    "#             데이터의 중간 50% 범위를 나타냄\n",
    "#         동작: v에서 v.quantile(0.75) - v.quantile(0.25)을 계산하여 IQR을 구함\n",
    "#     lambda v: skew(v) \n",
    "#         의미: 그룹별 데이터의 왜도(Skewness)를 계산.\n",
    "#             왜도는 데이터의 분포가 대칭적인지 아니면 한쪽으로 치우쳐 있는지를 보여 줌\n",
    "#         동작: v에 대해 skew(v) 함수를 사용하여 왜도를 계산\n",
    "#     lambda v: kurtosis(v)\n",
    "#         의미: 그룹별 데이터의 첨도(Kurtosis)를 계산.\n",
    "#             첨도는 데이터의 분포가 뾰족한지 또는 평평한지를 나타냄.\n",
    "#         동작: v에 대해 kurtosis(v) 함수를 사용하여 첨도를 계산.\n",
    "#     lambda v: v.max() - v.min()\n",
    "#         의미: 데이터의 범위(Range)를 계산. \n",
    "#             범위는 데이터의 최대값에서 최소값을 뺀 값으로, 데이터의 전체 변동 폭을 나타냄.\n",
    "#         동작: v의 최대값 v.max()에서 최소값 v.min()을 빼서 범위를 계산\n",
    "\n",
    "\n",
    "#     # 열 이름을 직관적으로 변경\n",
    "#     stats.columns = [\n",
    "#         \"Max\", \"Min\", \"Mean\", \"Variance\", \"Count\", \n",
    "#         \"Median\", \"Q1\", \"Q3\", \"IQR\", \n",
    "#         \"Skewness\", \"Kurtosis\", \"Range\"\n",
    "#     ]\n",
    "#     return stats\n",
    "# 해석 : \n",
    "# 통계치를 계산한 데이터프레임인 stats의 컬럼명을 명시하여 가독성을 높임\n",
    "# lambad로 계산된 부분은 컬럼명이 lambda_0 형식으로 되어 어떤 자료인지 가독성이 떨어짐\n",
    "# 함수로 계산된 통계치(stats)를 리턴으로 반환하기\n",
    "\n",
    "# df_year_grade_stats = group_analysis_by_two_columns(\"year\", \"grade\", \"fine_dust(㎍/㎥)\")\n",
    "# df_year_season_stats = group_analysis_by_two_columns(\"year\", \"season\", \"fine_dust(㎍/㎥)\")\n",
    "# df_year_day_stats = group_analysis_by_two_columns(\"year\", \"merge_day\", \"fine_dust(㎍/㎥)\")\n",
    "# df_grade_season_stats = group_analysis_by_two_columns(\"grade\", \"season\", \"fine_dust(㎍/㎥)\")\n",
    "# df_grade_day_stats = group_analysis_by_two_columns(\"grade\", \"merge_day\", \"fine_dust(㎍/㎥)\")\n",
    "# df_season_day_stats = group_analysis_by_two_columns(\"season\", \"merge_day\", \"fine_dust(㎍/㎥)\")\n",
    "# 해석 : \n",
    "# 위의 함수를 써서 그룹화된 2개의 컬럼을 통해 계산된 타겟의 통계치를,\n",
    "# 그룹별로 저장하는 데이터 프레임을 생성\n",
    "# 그룹별통계치 = 위에서 정의된 함수의 변수 명(전달인자 3개가 무엇인지 명시)\n",
    "# 예를 들어 (\"year\", \"grade\", \"fine_dust(㎍/㎥)\")는, \n",
    "# (group_col1=\"year\", group_col2=\"grade\", target_col=\"fine_dust(㎍/㎥)\")\n",
    "# 라고 전달인자를 명시하여 가독성을 높이게끔 코드를 쓸 수도 있음\n",
    "\n",
    "\n",
    "# # 결과를 딕셔너리로 정리\n",
    "# double_group_stats = {\n",
    "#     \"Year-Grade\": df_year_grade_stats,\n",
    "#     \"Year-Season\": df_year_season_stats,\n",
    "#     \"Year-Day\": df_year_day_stats,\n",
    "#     \"Grade-Season\": df_grade_season_stats,\n",
    "#     \"Grade-Day\": df_grade_day_stats,\n",
    "#     \"Season-Day\": df_season_day_stats\n",
    "# }\n",
    "# 해석 : \n",
    "# double_group_stats라는 딕셔너리를 생성\n",
    "# \"key\":value 형태로, key 부분에는 그룹된 2개 컬럼명, \n",
    "# value에는 함수를 사용해서 만든 그룹별 통계치 값들을 저장하기\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"\\n\\n2. 이중 그룹 분석\")\n",
    "# for key, value in double_group_stats.items():\n",
    "# 해석 : \n",
    "# for문에서 사용할 key와 value라는 변수를 만들고, \n",
    "# 각 변수들은 위에서 만든 딕셔너리의 값을 .items()를 써서, \n",
    "# 각각 for문의 key에는 double_group_stats의 key가, \n",
    "# for문의 value에는 double_group_stats의 value가 들어가게 만듦\n",
    "\n",
    "#     print(f\"\\n{key} Stats:\\n\", value)\n",
    "# 해석 : \n",
    "# f와 \"{key}\"를 써서, 위 for문에서 반복한 값을 출력하게 함\n",
    "# 이때, print(f\"\\n{key} Stats:\\n\", value) 대신,\n",
    "# print(\"\\n\", key, \"Stats:\\n\", value)로 써서 f\"{}\" 없이 \n",
    "# for문을 사용해 반복한 값들이 출력되게 하는 것이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 함수에서 개선 부분\n",
    "# # 튜플로 하면 통계치 계산과 컬럼이름을 한번에 지정할 수 있다.\n",
    "#     # 통계 계산 및 컬럼 이름 지정\n",
    "#     stats = grouped_df[target_col].agg([\n",
    "#     (\"Max\", \"max\"),                    # (열 이름, 계산 방법)\n",
    "#     (\"Min\", \"min\"),\n",
    "#     (\"Mean\", \"mean\"),\n",
    "#     (\"Variance\", \"var\"),\n",
    "#     (\"Count\", \"count\"),\n",
    "#     (\"Median\", \"median\"),                   # 중앙값\n",
    "#     (\"Q1\", lambda x: x.quantile(0.25)),     # 1사분위수 (Q1)\n",
    "#     (\"Q3\", lambda x: x.quantile(0.75)),     # 3사분위수 (Q3)\n",
    "#     (\"IQR\", lambda x: x.quantile(0.75) - x.quantile(0.25)),     # IQR\n",
    "#     (\"Skewness\", lambda x: skew(x)),        # 왜도\n",
    "#     (\"Kurtosis\", lambda x: kurtosis(x)),    # 첨도\n",
    "#     (\"Range\", lambda x: x.max() - x.min())  # 범위\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딕셔너리를 csv파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_stats_with_check(combined_stats, double_group_stats, triple_group_stats, quadruple_group_stats):\n",
    "#     print(\"데이터 구조 확인 중...\")\n",
    "#     print_data_structure(combined_stats, \"combined_stats\")\n",
    "#     print_data_structure(double_group_stats, \"double_group_stats\")\n",
    "#     print_data_structure(triple_group_stats, \"triple_group_stats\")\n",
    "#     print_data_structure(quadruple_group_stats, \"quadruple_group_stats\")\n",
    "    \n",
    "#     print(\"\\n저장 시작...\")\n",
    "#     save_multiindex_stats_to_csv(\n",
    "#         combined_stats,\n",
    "#         double_group_stats,\n",
    "#         triple_group_stats,\n",
    "#         quadruple_group_stats\n",
    "#     )\n",
    "\n",
    "# def print_data_structure(data, name):\n",
    "#     \"\"\"데이터 구조를 출력하여 확인\"\"\"\n",
    "#     print(f\"{name} 구조:\")\n",
    "#     if isinstance(data, dict):\n",
    "#         print({key: type(value) for key, value in data.items()})\n",
    "#     else:\n",
    "#         print(f\"{name} is {type(data)}\")\n",
    "\n",
    "# def save_multiindex_stats_to_csv(*args):\n",
    "#     \"\"\"각 딕셔너리 내 DataFrame을 개별 CSV로 저장\"\"\"\n",
    "#     for i, stats in enumerate(args, start=1):\n",
    "#         if isinstance(stats, dict):\n",
    "#             for key, df in stats.items():\n",
    "#                 if isinstance(df, pd.DataFrame):\n",
    "#                     filename = f\"stats_{i}_{key}.csv\"\n",
    "#                     df.to_csv(filename)\n",
    "#                     print(f\"{filename} 파일로 저장되었습니다.\")\n",
    "#                 else:\n",
    "#                     print(f\"{key}는 DataFrame 형식이 아니므로 CSV로 저장할 수 없습니다.\")\n",
    "#         else:\n",
    "#             print(f\"stats_{i}는 딕셔너리 형식이 아닙니다.\")\n",
    "\n",
    "# # 예제 실행 (실제 데이터로 교체)\n",
    "# save_stats_with_check(combined_stats, double_group_stats, triple_group_stats, quadruple_group_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해야 할 것 \n",
    "\n",
    "# 1-1. 미세먼지 시각화\n",
    "# 1-1-1. 단일 그룹\n",
    "# 1-1-2. 2개 그룹\n",
    "# 1-1-3. 3개 그룹\n",
    "# 1-1-4. 모든 그룹\n",
    "\n",
    "# 2. 강수량\n",
    "# 2-1. 강수량 분석\n",
    "# 2-2. 강수량 시각화\n",
    "\n",
    "# 3. 적설량\n",
    "# 3-1. 적설량 분석\n",
    "# 3-2. 적설량 시각화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딕서녀리를 csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def save_stats_with_check(combined_stats, double_group_stats, triple_group_stats, quadruple_group_stats):\n",
    "#     print(\"데이터 구조 확인 중...\")\n",
    "#     print_data_structure(combined_stats, \"combined_stats\")\n",
    "#     print_data_structure(double_group_stats, \"double_group_stats\")\n",
    "#     print_data_structure(triple_group_stats, \"triple_group_stats\")\n",
    "#     print_data_structure(quadruple_group_stats, \"quadruple_group_stats\")\n",
    "    \n",
    "#     print(\"\\n저장 시작...\")\n",
    "#     save_multiindex_stats_to_csv(\n",
    "#         combined_stats,\n",
    "#         double_group_stats,\n",
    "#         triple_group_stats,\n",
    "#         quadruple_group_stats\n",
    "#     )\n",
    "\n",
    "# def print_data_structure(data, name):\n",
    "#     \"\"\"데이터 구조를 출력하여 확인\"\"\"\n",
    "#     print(f\"{name} 구조:\")\n",
    "#     if isinstance(data, dict):\n",
    "#         print({key: type(value) for key, value in data.items()})\n",
    "#     else:\n",
    "#         print(f\"{name} is {type(data)}\")\n",
    "\n",
    "# def save_multiindex_stats_to_csv(*args):\n",
    "#     \"\"\"각 딕셔너리 내 DataFrame을 개별 CSV로 저장\"\"\"\n",
    "#     # Set the directory path\n",
    "#     directory = \"../dict_to_csv/\"\n",
    "#     # Create the directory if it does not exist\n",
    "#     os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "#     for i, stats in enumerate(args, start=1):\n",
    "#         if isinstance(stats, dict):\n",
    "#             for key, df in stats.items():\n",
    "#                 if isinstance(df, pd.DataFrame):\n",
    "#                     filename = os.path.join(directory, f\"stats_{i}_{key}.csv\")\n",
    "#                     df.to_csv(filename)\n",
    "#                     print(f\"{filename} 파일로 저장되었습니다.\")\n",
    "#                 else:\n",
    "#                     print(f\"{key}는 DataFrame 형식이 아니므로 CSV로 저장할 수 없습니다.\")\n",
    "#         else:\n",
    "#             print(f\"stats_{i}는 딕셔너리 형식이 아닙니다.\")\n",
    "\n",
    "# # 예제 실행 (실제 데이터로 교체)\n",
    "# save_stats_with_check(combined_stats, double_group_stats, triple_group_stats, quadruple_group_stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-da-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
